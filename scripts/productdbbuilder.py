import pandas as pd
import json
from tqdm import tqdm
from sentence_transformers import SentenceTransformer
import chromadb
import numpy as np

# === Config ===
CSV_PATH = "all_chewy_products_with_qanda.csv"
REVIEW_SYNTH_PATH = "results.jsonl"

COLLECTION_NAME = "products"
EMBEDDING_MODEL = "all-MiniLM-L6-v2"
CHROMADB_PATH = "chroma_db"
BATCH_SIZE = 1000

# === Step 1: Load Data ===
df = pd.read_csv(
    CSV_PATH,
    dtype={
        "PRODUCT_ID": str,
        "PART_NUMBER": str,
        "PARENT_ID": str,
        "PARENT_PART_NUMBER": str
    }
)

# === Step 2: Build Item Variant Map ===
df_items = df[df['TYPE'] == 'Item']
items_grouped = df_items.groupby('PARENT_ID')[['NAME', 'PART_NUMBER', 'PRODUCT_ID']].agg(list).to_dict(orient='index')

items_dict = {
    parent_id: [
        {"NAME": n, "PART_NUMBER": p, "PRODUCT_ID": pid}
        for n, p, pid in zip(vals["NAME"], vals["PART_NUMBER"], vals["PRODUCT_ID"])
    ]
    for parent_id, vals in items_grouped.items()
}

# Get first available images for each parent product from items
images_grouped = df_items.groupby('PARENT_ID')[['THUMBNAIL', 'FULLIMAGE']].agg(lambda x: x.dropna().iloc[0] if x.dropna().size > 0 else None).to_dict(orient='index')
images_dict = {
    parent_id: {
        "THUMBNAIL": vals["THUMBNAIL"],
        "FULLIMAGE": vals["FULLIMAGE"]
    }
    for parent_id, vals in images_grouped.items()
}

# === Step 3: Filter for Parent Products ===
product_df = df[df['TYPE'] == 'Product'].fillna({
    "NAME": "",
    "CLEAN_NAME": "",
    "DESCRIPTION_LONG": "",
    "CATEGORY_LEVEL1": "",
    "CATEGORY_LEVEL2": "",
    "CATEGORY_LEVEL3": "",
    "PRICE": 0.0,
    "AUTOSHIP_PRICE": 0.0,
    "RATING_AVG": 0.0,
    "RATING_CNT": 0.0,
    "ATTR_PET_TYPE": "",
    "ATTR_FOOD_FORM": "",
    "ATTR_SPECIAL_DIET": "",
    "INGREDIENTS": "",
    "THUMBNAIL": "",
    "FULLIMAGE": "",
    "PURCHASE_BRAND": "",
})

# === Step 3.5: Fill Missing Thumbnails and Print Stats ===
# Track NaNs before filling
nan_before = product_df['THUMBNAIL'].isna().sum() + (product_df['THUMBNAIL'] == '').sum()

# Fill thumbnails from images_dict (based on item variants)
def fill_image(row, col):
    if not row[col]:  # empty string or NaN
        return images_dict.get(row['PRODUCT_ID'], {}).get(col, "")
    return row[col]

product_df['THUMBNAIL'] = product_df.apply(lambda row: fill_image(row, 'THUMBNAIL'), axis=1)
product_df['FULLIMAGE'] = product_df.apply(lambda row: fill_image(row, 'FULLIMAGE'), axis=1)

# Track NaNs after filling
nan_after = product_df['THUMBNAIL'].isna().sum() + (product_df['THUMBNAIL'] == '').sum()

print(f"üñºÔ∏è THUMBNAILs missing before fill: {nan_before}")
print(f"üñºÔ∏è THUMBNAILs missing after fill: {nan_after}")


# === Step 4: Load Review Synthesis JSONL ===
with open(REVIEW_SYNTH_PATH, "r") as f:
    review_map = {
        entry["product_id"]: entry
        for entry in map(json.loads, f)
    }

# === Step 5: Build Documents & Metadata ===
default_synth = {
    "what_customers_love": ["Insufficient reviews! No review synthesis"],
    "what_to_watch_out_for": ["Insufficient reviews! No review synthesis"],
    "should_you_buy_it": "Insufficient reviews! No review synthesis"
}

product_rows = product_df.to_dict(orient="records")
documents = []
metadatas = []
ids = []

for row in product_rows:
    part_number = row["PART_NUMBER"]
    product_id = row["PRODUCT_ID"]
    
    # Ensure we have valid text for embedding
    clean_name = row["CLEAN_NAME"] or row["NAME"] or f"Product {product_id}"
    if not clean_name or clean_name.strip() == "":
        clean_name = f"Product {product_id}"

    # add purchase brand to clean name if available: Brand: {row['PURCHASE_BRAND']}
    if row["PURCHASE_BRAND"]:
        clean_name += f" | Brand: {row['PURCHASE_BRAND']}"

    metadata = {
        "PRODUCT_ID": product_id,
        "PART_NUMBER": part_number,
        "NAME": row["NAME"],
        "CLEAN_NAME": row["CLEAN_NAME"],
        "PURCHASE_BRAND": row["PURCHASE_BRAND"],
        "CATEGORY_LEVEL1": row["CATEGORY_LEVEL1"],
        "CATEGORY_LEVEL2": row["CATEGORY_LEVEL2"],
        "CATEGORY_LEVEL3": row["CATEGORY_LEVEL3"],
        "PRICE": row["PRICE"],
        "AUTOSHIP_PRICE": row["AUTOSHIP_PRICE"],
        "RATING_AVG": row["RATING_AVG"],
        "RATING_CNT": row["RATING_CNT"],
        "DESCRIPTION_LONG": row["DESCRIPTION_LONG"],
        "THUMBNAIL": row["THUMBNAIL"] if row["THUMBNAIL"] else images_dict.get(part_number, {}).get("THUMBNAIL", ""),
        "FULLIMAGE": row["FULLIMAGE"] if row["FULLIMAGE"] else images_dict.get(part_number, {}).get("FULLIMAGE", ""),
        "ATTR_PET_TYPE": row["ATTR_PET_TYPE"],
        "ATTR_FOOD_FORM": row["ATTR_FOOD_FORM"],
        "items": json.dumps(items_dict.get(product_id, [])),
        "review_synthesis": json.dumps(review_map.get(part_number, default_synth)),
        "review_synthesis_flag": part_number in review_map,
    }

    # Add special diet tags
    if row["ATTR_SPECIAL_DIET"]:
        for tag in map(str.strip, row["ATTR_SPECIAL_DIET"].split(',')):
            if tag:
                metadata[f"specialdiettag:{tag}"] = True

    # Add ingredient tags
    if row["INGREDIENTS"]:
        for tag in map(str.strip, row["INGREDIENTS"].split(',')):
            if tag:
                metadata[f"ingredienttag:{tag}"] = True

    documents.append(clean_name)
    metadatas.append(metadata)
    ids.append(product_id)

print(f"‚úÖ Prepared {len(documents)} documents for embedding")

# === Step 6: Initialize ChromaDB & Embed ===
print("üîÑ Loading embedding model...")
model = SentenceTransformer(EMBEDDING_MODEL)

print("üîÑ Initializing ChromaDB...")
client = chromadb.PersistentClient(path=CHROMADB_PATH)

# Delete existing collection if it exists
try:
    client.delete_collection(name=COLLECTION_NAME)
    print(f"üóëÔ∏è  Deleted existing collection: {COLLECTION_NAME}")
except:
    pass

# Create new collection with proper embedding function
collection = client.create_collection(
    name=COLLECTION_NAME,
    metadata={"hnsw:space": "cosine"}
)

print(f"‚úÖ Created collection: {COLLECTION_NAME}")

# === Step 7: Batch Upload ===
print("üîÑ Starting batch embedding and upload...")
total_processed = 0

for i in tqdm(range(0, len(documents), BATCH_SIZE), desc="Processing batches"):
    doc_batch = documents[i:i + BATCH_SIZE]
    meta_batch = metadatas[i:i + BATCH_SIZE]
    id_batch = ids[i:i + BATCH_SIZE]

    try:
        # Generate embeddings
        emb_batch = model.encode(doc_batch, batch_size=32, show_progress_bar=False)
        
        # Convert to list format for ChromaDB
        emb_batch_list = emb_batch.tolist()
        
        # Validate embeddings
        if len(emb_batch_list) != len(doc_batch):
            print(f"‚ùå Embedding count mismatch: {len(emb_batch_list)} vs {len(doc_batch)}")
            continue
            
        # Upload to ChromaDB
        collection.upsert(
            documents=doc_batch,
            embeddings=emb_batch_list,
            metadatas=meta_batch,
            ids=id_batch,
        )
        
        total_processed += len(doc_batch)
        
    except Exception as e:
        print(f"‚ùå Error processing batch {i//BATCH_SIZE + 1}: {str(e)}")
        continue

print(f"‚úÖ Successfully processed {total_processed} documents")

# Verify the upload
try:
    count = collection.count()
    print(f"‚úÖ ChromaDB collection now contains {count} documents")
    
    # Test a query
    if count > 0:
        results = collection.query(
            query_texts=["pet food"],
            n_results=1
        )
        print(f"‚úÖ Test query successful, found {len(results['ids'][0])} results")
        
except Exception as e:
    print(f"‚ùå Error verifying upload: {str(e)}")

print("‚úÖ Embedding process complete!")